{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ohlDhP9yB4KZ"
   },
   "source": [
    "# DTSC670: Foundations of Machine Learning Models\n",
    "## Assignment 4: Custom Transformer and Transformation Pipeline\n",
    "\n",
    "#### Name:\n",
    "\n",
    "### CodeGrade\n",
    "Note that this assignment will be automatically graded through CodeGrade and you will have unlimited submission attempts.  When submitting to CodeGrade, your notebook should be named `assignment4.ipynb` and there should be no errors in the file or CodeGrade will not be able to grade it.  Before submitting, I suggest that you restart your kernel and attempt to run all cells again to ensure that there will be no errors when CodeGrade runs your script.\n",
    "\n",
    "### Details\n",
    "Your task in this assignment is to create a custom transformation pipeline that takes in raw data and returns fully prepared, clean data that is ready for model training.  However, we will not actually train any models in this assignment.  This pipeline will employ an imputer class, a user-defined transformer class, and a data-normalization class.\n",
    "\n",
    "Please note that the order of features in the final feature matrix must be correct.  See the below figure that illustrates the input and output of the transformation pipeline.  The positions of features $x_1$ and $x_2$ do not change - they remain in the first and second columns, respectively, both before and after the transformation pipeline.  In the transformed dataset, the $x_5$ feature is next, and is followed by the newly computed feature $x_6$.  Finally, the last two columns are the remaining one-hot vectors obtained from encoding the categorical feature $x_3$.\n",
    "\n",
    "<img src=\"DataTransformation.png \" width =\"500\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "paEI44LpB4Kg"
   },
   "source": [
    "# Import Data\n",
    "\n",
    "- Make sure that your notebook and data file are located in the same folder since this is what CodeGrade will be expecting.  \n",
    "- Import the data from the file called `CustomTransformerData.csv` and call this DataFrame `custom_transform`.  \n",
    "- Output the `custom_transform` DataFrame to see the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 591
    },
    "executionInfo": {
     "elapsed": 150,
     "status": "ok",
     "timestamp": 1624045785118,
     "user": {
      "displayName": "Ben Pickard",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgPExDuYgUieHlogH8E532au3146lVrJ6Ixni9u=s64",
      "userId": "12452112282107072949"
     },
     "user_tz": 240
    },
    "id": "iviPNPMhB4Kh",
    "outputId": "67710931-03a9-494d-c85f-e7095ab12143"
   },
   "outputs": [],
   "source": [
    "### ENTER CODE HERE ###\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#Import data from Excel\n",
    "custom_transform = pd.read_csv('CustomTransformerData.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nopVKUnbB4Kt"
   },
   "source": [
    "# Create Numeric and Categorical DataFrames\n",
    "\n",
    "- Create two new DataFrames: \n",
    "  - Create one DataFrame called `data_num` that holds the numeric features from the `custom_transform` DataFrame.  \n",
    "  - Create another DataFrame called `data_cat` that holds the categorical feature from `custom_transform`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1624045786487,
     "user": {
      "displayName": "Ben Pickard",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgPExDuYgUieHlogH8E532au3146lVrJ6Ixni9u=s64",
      "userId": "12452112282107072949"
     },
     "user_tz": 240
    },
    "id": "qj5GZ5YiB4Ku"
   },
   "outputs": [],
   "source": [
    "### ENTER CODE HERE ###\n",
    "\n",
    "data_num = custom_transform[['x1', 'x2', 'x4', 'x5']]\n",
    "\n",
    "data_cat = custom_transform[['x3']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ENTER CODE HERE ###\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ASUFXY7BB4Km"
   },
   "source": [
    "# Create Custom Transformer\n",
    "\n",
    "Create a custom transformer, just as we did in the lecture video entitled \"Custom Transformers\", that performs two computations: \n",
    "\n",
    "1. Adds an attribute to the end of the numerical data (i.e. new last column) that is equal to $\\frac{x_1^3}{x_5}$ for each observation.  In other words, for each instance, you will cube the $x_1$ column and then divide by the $x_5$ column.\n",
    "\n",
    "2. Drops the entire $x_4$ feature column if the passed function argument `drop_x4` is `True` and doesn't drop the column if `drop_x4` is `False`. (See further instructions below.)\n",
    "\n",
    "You must name your custom transformer class `Assignment4Transformer`. Your class should include an input parameter called `drop_x4` with a default value of `True` that deletes the $x_4$ feature column when its value is `True`, but preserves the $x_4$ feature column when you pass a value of `False`.\n",
    "\n",
    "This transformer will be used in a pipeline. In that pipeline, an imputer will be run *before* this transformer. Keep in mind that the imputer will output an array, so **this transformer must be written to accept an array.**  This is very important and a cause of many errors that students encounter.  In other words, think about using NumPy in your transformer instead of Pandas.\n",
    "\n",
    "Additionally, this transformer will ONLY be given the numerical features of the data. The categorical feature will be handled elsewhere in the full pipeline. This means that your code for this transformer **must reflect the absence of the categorical $x_3$ column** when indexing data structures.  Again this is very important and a cause of the second most number of errors that students encounter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 848,
     "status": "ok",
     "timestamp": 1624045786162,
     "user": {
      "displayName": "Ben Pickard",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgPExDuYgUieHlogH8E532au3146lVrJ6Ixni9u=s64",
      "userId": "12452112282107072949"
     },
     "user_tz": 240
    },
    "id": "l5UgwTFtB4Kp"
   },
   "outputs": [],
   "source": [
    "### ENTER CODE HERE ###\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class Assignment4Transformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, drop_x4=True):\n",
    "        self.drop_x4 = drop_x4       \n",
    "    def fit(self, X, y=None):\n",
    "        return self   \n",
    "    def transform(self, X):\n",
    "        x6 = (X[:,0]**3) / (X[:,3])\n",
    "        if self.drop_x4 == True:\n",
    "            X = np.delete(X, 2, axis=1)\n",
    "        return np.c_[X,x6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kzTuITvAB4Kr"
   },
   "source": [
    "# Create Transformation Pipeline for Numerical Features\n",
    "\n",
    "Create a custom transformation pipeline for only the numeric data called `num_pipeline` that:\n",
    "\n",
    "1. Applies the `SimpleImputer` class to the data, where the strategy is set to `mean`.  The name of this step should be \"imputer\".\n",
    "\n",
    "2. Applies the custom `Assignment4Transformer` class to the data.  Make sure that your custom transformer uses the default argument where you drop the $x_4$ column.  The name of this step should be \"custom_trans\".\n",
    "\n",
    "3. Applies the `StandardScaler` class to the data.  The name of this step should be \"std_scaler\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "executionInfo": {
     "elapsed": 331,
     "status": "ok",
     "timestamp": 1624045786486,
     "user": {
      "displayName": "Ben Pickard",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgPExDuYgUieHlogH8E532au3146lVrJ6Ixni9u=s64",
      "userId": "12452112282107072949"
     },
     "user_tz": 240
    },
    "id": "co6dm-JAB4Ks"
   },
   "outputs": [],
   "source": [
    "### ENTER CODE HERE ###\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "num_pipeline = Pipeline([('imputer', SimpleImputer(strategy='mean')), \n",
    "                        ('custom_trans', Assignment4Transformer()),\n",
    "                        ('std_scaler', StandardScaler())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yLDPD7faB4Kv"
   },
   "source": [
    "# Quick Testing\n",
    "\n",
    "The full pipeline will be implemented with a `ColumnTransformer` class.  However, to be sure that our numeric pipeline is working properly, lets invoke the `fit_transform()` method of the `num_pipeline` object passing it your `data_num` DataFrame.  Save this output data into a variable called `data_num_trans`.\n",
    "\n",
    "### Run Pipeline and Create Transformed Numeric Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1624045786493,
     "user": {
      "displayName": "Ben Pickard",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgPExDuYgUieHlogH8E532au3146lVrJ6Ixni9u=s64",
      "userId": "12452112282107072949"
     },
     "user_tz": 240
    },
    "id": "PJknn8GUB4Kw",
    "outputId": "29b16d57-08df-421d-b21a-0c061be1ea20",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### ENTER CODE HERE ###\n",
    "data_num_trans = num_pipeline.fit_transform(data_num)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "18MuMZi_B4Kx"
   },
   "source": [
    "### One-Hot Encode Categorical Features\n",
    "\n",
    "Similarly, you will employ a `OneHotEncoder` class in the `ColumnTransformer` below to construct the final full pipeline.  However, let's instantiate an object of the `OneHotEncoder` class to use in the `ColumnTransformer`.\n",
    "\n",
    "1. Call this object `cat_encoder` that has the `drop` parameter set to `first`.  \n",
    "2. Also, include `sparse=False` when instantiating the object to make the auto grading work correctly.  \n",
    "3. Next, call the `fit_transform()` method and pass it your categorical data (`data_cat`).  \n",
    "4. Save this output data into a variable called `data_cat_OHE`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ENTER CODE HERE ###\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "cat_encoder = OneHotEncoder(drop='first', sparse=False)\n",
    "data_cat_OHE = cat_encoder.fit_transform(data_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NuUbjOPiB4K0"
   },
   "source": [
    "# Put it All Together with a Column Transformer\n",
    "\n",
    "Now, we are finally ready to construct the full transformation pipeline called `full_pipeline` that will transform our raw data into clean, ready-to-train data.  \n",
    "1. Construct this `ColumnTransformer` below.  Note that you will use:\n",
    "  - the `num_pipeline` in the full_pipeline with your numeric data\n",
    "  - the `cat_encoder` object in the full_pipeline with your categorical data\n",
    "2. Call the `fit_transform()` method on the original `custom_transform` data to obtain the final, clean data.  \n",
    "3. Save this output data into a variable called `data_trans`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "executionInfo": {
     "elapsed": 131,
     "status": "ok",
     "timestamp": 1624045797883,
     "user": {
      "displayName": "Ben Pickard",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgPExDuYgUieHlogH8E532au3146lVrJ6Ixni9u=s64",
      "userId": "12452112282107072949"
     },
     "user_tz": 240
    },
    "id": "7m2SuGOpB4K2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### ENTER CODE HERE ###\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "num_attribs = list(data_num)\n",
    "cat_attribs = list(data_cat)\n",
    "full_pipeline = ColumnTransformer([\n",
    "    (\"num\", num_pipeline, num_attribs),\n",
    "    ('cat', cat_encoder, cat_attribs)\n",
    "])\n",
    "\n",
    "data_trans = full_pipeline.fit_transform(custom_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AqcJ5AjtB4K3"
   },
   "source": [
    "# Prepare for Grading\n",
    "\n",
    "Prepare your `data_trans` NumPy array for grading by running the code below.  We are using the NumPy [around()](https://numpy.org/doc/stable/reference/generated/numpy.around.html) function to round all the values to 2 decimal places and this will return a NumPy array.\n",
    "\n",
    "Please double check that the final order of the features in this `data_trans` array matches what was given to you at the top of this document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "aborted",
     "timestamp": 1624045786716,
     "user": {
      "displayName": "Ben Pickard",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgPExDuYgUieHlogH8E532au3146lVrJ6Ixni9u=s64",
      "userId": "12452112282107072949"
     },
     "user_tz": 240
    },
    "id": "RME1nK-TB4K5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.64, -1.73, -1.2 , -1.59,  0.  ,  0.  ],\n",
       "       [-1.45, -1.53, -1.16, -1.4 ,  0.  ,  1.  ],\n",
       "       [-1.25, -1.38, -1.1 , -1.21,  0.  ,  0.  ],\n",
       "       [-1.06,  0.  , -1.03, -1.02,  0.  ,  0.  ],\n",
       "       [-0.87, -0.99, -0.93, -0.83,  0.  ,  1.  ],\n",
       "       [-0.67, -0.7 , -0.82, -0.64,  0.  ,  1.  ],\n",
       "       [-0.48, -0.53, -0.69, -0.45,  1.  ,  0.  ],\n",
       "       [-0.29, -0.28, -0.54, -0.26,  0.  ,  0.  ],\n",
       "       [-0.1 , -0.04,  0.  , -0.61,  0.  ,  1.  ],\n",
       "       [ 0.1 ,  0.13, -0.18,  0.13,  1.  ,  0.  ],\n",
       "       [ 0.29,  0.27,  0.03,  0.32,  0.  ,  1.  ],\n",
       "       [ 0.48,  0.45,  0.26,  0.51,  0.  ,  1.  ],\n",
       "       [ 0.67,  0.76,  0.5 ,  0.7 ,  0.  ,  0.  ],\n",
       "       [ 0.87,  0.88,  0.76,  0.89,  1.  ,  0.  ],\n",
       "       [ 1.06,  0.  ,  1.05,  1.08,  1.  ,  0.  ],\n",
       "       [ 1.25,  1.42,  1.35,  1.27,  0.  ,  1.  ],\n",
       "       [ 1.45,  1.55,  1.67,  1.46,  1.  ,  0.  ],\n",
       "       [ 1.64,  1.71,  2.01,  1.65,  1.  ,  0.  ]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data_trans = np.around(data_trans, decimals=2)\n",
    "\n",
    "data_trans"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment_4_complete.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
